{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/homebrew/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/lib/python3.11/site-packages (from requests) (2024.2.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/homebrew/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed and saved to website_contents.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "import time\n",
    "\n",
    "# List of URLs to start scraping from\n",
    "start_urls = [\n",
    "    'https://baylegal.org/',\n",
    "    'https://www.lawhelpca.org/',\n",
    "    'https://www.cuav.org/',\n",
    "    'https://www.americanbar.org/groups/domestic_violence/',\n",
    "    'https://www.cpedv.org/',\n",
    "    'https://www.cpedv.org/bay-area-region',\n",
    "    'http://www.feminist.org/911/crisis.html',\n",
    "    'https://www.columbusfamilylaw.org/news/understanding-the-signs-of-domestic-violence/',\n",
    "    'https://baylegal.org/get-help/resources/fair-housing-resources/',\n",
    "    'https://www.hud.gov/program_offices/fair_housing_equal_opp/online-complaint',\n",
    "    'https://calcivilrights.ca.gov/complaintprocess/',\n",
    "    'https://www.marincounty.org/-/media/files/departments/da/consumer-guides/tenantshandbook.pdf',\n",
    "    'https://calcivilrights.ca.gov/',\n",
    "    'https://evictiondefense.org/',\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # Add more starting URLs as needed\n",
    "]\n",
    "\n",
    "# Maximum number of pages to scrape per website\n",
    "max_pages = 5\n",
    "max_retries = 3\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Function to scrape content from a given URL with headers\n",
    "def scrape_website(url):\n",
    "    try:\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers)\n",
    "                response.raise_for_status()  # Check for request errors\n",
    "                break  # If the request is successful, exit the retry loop\n",
    "            except requests.RequestException as e:\n",
    "                if attempt < max_retries - 1:  # Retry only if attempts are remaining\n",
    "                    print(f\"Retrying {url}: attempt {attempt + 1}\")\n",
    "                    time.sleep(2)  # Wait before retrying\n",
    "                else:\n",
    "                    raise  # If no attempts are left, raise the exception\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extracting the text content\n",
    "        content = soup.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        return {\n",
    "            'url': url,\n",
    "            'content': content,\n",
    "            'links': [urljoin(url, a['href']) for a in soup.find_all('a', href=True)]\n",
    "        }\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return {\n",
    "            'url': url,\n",
    "            'content': None,\n",
    "            'links': [],\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Function to crawl a website starting from the given URL\n",
    "def crawl_website(start_url, max_pages):\n",
    "    to_visit = [start_url]\n",
    "    visited = set()\n",
    "    website_contents = []\n",
    "\n",
    "    while to_visit and len(website_contents) < max_pages:\n",
    "        current_url = to_visit.pop(0)\n",
    "        if current_url not in visited:\n",
    "            content_dict = scrape_website(current_url)\n",
    "            website_contents.append(content_dict)\n",
    "            visited.add(current_url)\n",
    "            to_visit.extend([link for link in content_dict['links'] if link not in visited])\n",
    "\n",
    "            # Delay to prevent overwhelming the server\n",
    "            time.sleep(1)\n",
    "\n",
    "    return website_contents\n",
    "\n",
    "# Scraping each starting URL\n",
    "all_website_contents = []\n",
    "\n",
    "for start_url in start_urls:\n",
    "    website_contents = crawl_website(start_url, max_pages)\n",
    "    all_website_contents.extend(website_contents)\n",
    "\n",
    "for w in all_website_contents:\n",
    "    del w['links']\n",
    "# Save the contents to a JSON file\n",
    "with open('website_contents.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(all_website_contents, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Scraping completed and saved to website_contents.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
